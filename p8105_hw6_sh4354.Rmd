---
title: "Homework 6"
author: "Selina Hsuan"
date: '`r format(Sys.time(), "%Y-%m-%d")`'
output: github_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE, message = FALSE)
```

```{r}
library(tidyverse)
library(modelr)
library(mgcv)

set.seed(1)
```


## Problem 1

Import and clean dataset 
```{r}
homicide_df = 
  read_csv("data/homicide-data.csv", na = c("", "NA", "Unknown")) |> 
  mutate(
    city_state = str_c(city, state, sep = ", "),
    victim_age = as.numeric(victim_age),
    resolution = case_when(
      disposition == "Closed without arrest" ~ 0,
      disposition == "Open/No arrest"        ~ 0,
      disposition == "Closed by arrest"      ~ 1)
  ) |> 
  filter(victim_race %in% c("White", "Black")) |> 
  filter(!(city_state %in% c("Tulsa, AL", "Dallas, TX", "Phoenix, AZ", "Kansas City, MO"))) |> 
  select(city_state, resolution, victim_age, victim_sex, victim_race)
```


For Balimore, MD, fit logistic regression model and obtain estimate and CI of odds ratio comparing male to female victims
```{r}
baltimore_glm = 
  filter(homicide_df, city_state == "Baltimore, MD") |> 
  glm(resolution ~ victim_age + victim_sex + victim_race, family = binomial(), data = _)

baltimore_glm |> 
  broom::tidy() |> 
  mutate(
    OR = exp(estimate), 
    OR_CI_upper = exp(estimate + 1.96 * std.error),
    OR_CI_lower = exp(estimate - 1.96 * std.error)) |> 
  filter(term == "victim_sexMale") |> 
  select(OR, OR_CI_lower, OR_CI_upper) |>
  knitr::kable(digits = 3)
```

Conduct same analysis for each city
```{r}
model_results = 
  homicide_df |> 
  nest(data = -city_state) |> 
  mutate(
    models = map(data, \(df) glm(resolution ~ victim_age + victim_sex + victim_race, 
                             family = binomial(), data = df)),
    tidy_models = map(models, broom::tidy)) |> 
  select(-models, -data) |> 
  unnest(cols = tidy_models) |> 
  mutate(
    OR = exp(estimate), 
    OR_CI_upper = exp(estimate + 1.96 * std.error),
    OR_CI_lower = exp(estimate - 1.96 * std.error)) |> 
  filter(term == "victim_sexMale") |> 
  select(city_state, OR, OR_CI_lower, OR_CI_upper)

model_results |>
  slice(1:5) |> 
  knitr::kable(digits = 3)
```

Generate plot
```{r}
model_results |> 
  mutate(city_state = fct_reorder(city_state, OR)) |> 
  ggplot(aes(x = city_state, y = OR)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = OR_CI_lower, ymax = OR_CI_upper)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```



## Problem 2

First, we download Central Park weather data.

```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2022-01-01",
    date_max = "2022-12-31") |>
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) |>
  select(name, id, everything())
```


We create a simple linear regression model with `tmax` as the response and `tmin` and `prcp` as the predictors. 

```{r}
model = 
  weather_df |> 
  lm(tmax ~ tmin + prcp, data = _) |> 
  broom::glance() |> 
  select(r.squared)
```


Using a function, we create a data frame that contains the r^2 and log(β̂1∗β̂2) value from the simple linear regression model. We extract r^2 from the fitted regression using `broom:glance`, and we use `broom:tidy` to compute log(β̂1∗β̂2). If either `tmin` or `prcp` contain a negative value, the log value is recorded as "N/A."

```{r}
boot_sample = function(df) {
  
  data = sample_frac(df, replace = TRUE)
  
  output1 = data |> 
    lm(tmax ~ tmin + prcp, data = _) |> 
    broom::glance() |> 
    select(r.squared) 
  
  output2 = data |> 
  lm(tmax ~ tmin + prcp, data = _) |> 
  broom::tidy() |> 
  slice(c(2,3)) |> 
  select(term, estimate) |> 
  pivot_wider(
    names_from = term, values_from = estimate
    ) |> 
  mutate(log_betas = ifelse(tmin > 0 & prcp > 0, log(tmin * prcp), NA)) |> 
  select(log_betas) 
  
  tibble(
    output1,
    output2
  )
}
```


Now we produce estimates of the two quantities for each of 5000 bootstrap samples. 

```{r}
boot_straps = 
  tibble(strap_number = 1:5000) |> 
  mutate(
    strap_sample = map(strap_number, \(i) boot_sample(weather_df))
  ) |> 
  unnest(strap_sample)

```


We can create a plot of the distribution of r^2 values. 

```{r}
boot_straps |> 
  ggplot(aes(x = r.squared)) + 
  geom_density()
```


We can also create a plot of the distribution of log(β̂1∗β̂2̂) values. In this plot, out of 5000 values of log(β1*β2), 3331 were removed from the analysis because the value was "N/A".

```{r}
boot_straps |> 
  ggplot(aes(x = log_betas)) + 
  geom_density()
```


Finally, using the 5000 bootstrap estimates, we identify the 2.5% and 97.5% quantiles to provide a 95% confidence interval for r^2 and log(β̂1∗β̂2). 

```{r}
boot_straps |> 
  select(r.squared,log_betas) |> 
  pivot_longer(
    cols = c(r.squared, log_betas),
    names_to = "quantity", 
    values_to = "estimate"
  ) |> 
  group_by(quantity) |> 
  summarize(
    ci_lower = quantile(estimate, 0.025, na.rm = TRUE),
    ci_upper = quantile(estimate, 0.975, na.rm = TRUE)) |> 
  knitr::kable(digits = 4)
 
```




## Problem 3

First, we load and clean the birthweight dataset. 
```{r}
birthweight =
  read_csv("data/birthweight.csv") |> 
  janitor::clean_names() |> 
  mutate(babysex = as.factor(babysex))
```


Next, we propose a regression model for birthweight. I hypothesize that baby's length at birth (blength) and family monthly income (fincome) may be a good model for birth weight. The baby's length is likely directly related due birth weight because a larger baby weighs more. Family income may also be related to birth weight due to social determinants of health, in which families with lower income are more likely to have high blood pressure, diabetes, and heart, and other chronic health conditions, which may contribute to babies with lower weight. 

```{r}
proposed_mod = 
  birthweight |> 
  lm(bwt ~ blength + fincome, data = _)
```


Using `add_predictions` and `add_residuals`, we create a plot of model residuals against fitted values. 

```{r}
birthweight |> 
  modelr::add_predictions(proposed_mod) |> 
  modelr::add_residuals(proposed_mod) |> 
  ggplot(aes(x = pred, y = resid)) + 
  geom_point() + 
  labs(title = "Residuals vs. Fitted Values",
       x = "Fitted Values",
       y = "Residuals") 
```


Now we create two other regression models for comparisons. 

```{r}
comparison1_mod = 
  birthweight |> 
  lm(bwt ~ blength + gaweeks, data = _)

comparison2_mod = 
  birthweight |> 
  lm(bwt ~ babysex*bhead*blength, data = _)
```


Here is a plot of the three models. 

```{r}
birthweight |> 
  gather_predictions(proposed_mod, comparison1_mod, comparison2_mod) |> 
  mutate(model = fct_inorder(model)) |> 
  ggplot(aes(x = blength, y = bwt)) + 
  geom_point(alpha = .5) +
  geom_line(aes(y = pred), color = "red") + 
  facet_grid(~model)
```


Using `cross_mc`, we create a data frame of training and test data. 

```{r}
cv_df =
  crossv_mc(birthweight, 100) |> 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))
```

Next, we fit the candidate models and assess prediction accuracy by obtaining RMSE's using `mutate`, `map`, and `map2`.

```{r}
cv_df = 
  cv_df |> 
  mutate(
    model_mod  = map(train, \(df) lm(bwt ~ blength + fincome, data = df)),
    comparison1_mod     = map(train, \(df) lm(bwt ~ blength + gaweeks, data = df)),
    comparison2_mod  = map(train, \(df) lm(bwt ~ babysex*bhead*blength, data = df))) |> 
  mutate(
    rmse_model = map2_dbl(model_mod, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_comparison1    = map2_dbl(comparison1_mod, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_comparison2 = map2_dbl(comparison2_mod, test, \(mod, df) rmse(model = mod, data = df)))
```


The plot below shows the distribution of RMSE values for each candidate model.

```{r}
cv_df |> 
  select(starts_with("rmse")) |> 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") |> 
  mutate(model = fct_inorder(model)) |> 
  ggplot(aes(x = model, y = rmse)) + geom_violin()
```

Between the 3 regression models, the second comparison model with interactions has the lowest RSME distribution and thus the best predictive accuracy. The RSME distribution for the proposed model and the first comparison model are similar, although the minimum and maximum values are slightly higher for the proposed model. 
